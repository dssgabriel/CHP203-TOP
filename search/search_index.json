{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This site contains courses and practical work for the course CHP203: Parallel Optimization Techniques offered by Hugo Taboada and Gabriel Dos Santos as part of the High Performance Computing &amp; Simulation M.Sc. program at ISTY/UVSQ. Its use is reserved for students of the Paris-Saclay University.</p> <p>The Spring 2025 session will start on Tuesday, 25 March and end on Wednesday, 30 April.</p>"},{"location":"#course-information","title":"Course Information","text":""},{"location":"#description-objectives","title":"Description &amp; objectives","text":"<p>This course provides an overview of common optimization techniques for modern HPC architectures, as well as giving an introduction to supercomputing tools and environments.</p> <p>By the end of the course, the students are expected to have acquired a thorough understanding of the following concepts:</p> <ul> <li>Compilers, build system toolchains and HPC environments;</li> <li>Parallel and distributed programming models;</li> <li>Parallel and distributed profiling tools;</li> <li>Data visulization methodology for performance benchmarking;</li> <li>Memory and data structure layout optimizations;</li> <li>Shared-memory parallelism optimizations;</li> <li>CPU microarchitecture: vectorization, unrolling and ILP optimizations.</li> <li>Distributed programming optimizations;</li> </ul>"},{"location":"#lecture-sessions","title":"Lecture sessions","text":"<p>Tuesday, 13:45 - 17:00 CET, room CN2, Rabelais building, Guyancourt campus.</p>"},{"location":"#lab-sessions","title":"Lab sessions","text":"<p>Tuesday, 9:30 - 12:45 CET, room CN2, Rabelais building, Guyancourt campus.</p>"},{"location":"#examination","title":"Examination","text":"<p>This course is evaluated for the 1st Year of Paris-Saclay University's M.Sc. in High Performance Computing &amp; Simulation.</p> <p>Warning</p> <p>All lecture and lab sessions are mandatory. Any absence shall be justified to the administration and the course organizers.</p> ECTS credits 3 credits (out of 30 semester credits)"},{"location":"#staff-information","title":"Staff Information","text":"Role Name E-mail Instructor Hugo Taboada hugo.taboada@cea.fr Teaching assistant Gabriel Dos Santos gabriel.dossantos@cea.fr <p>Please contact the instructor or TA for any questions regarding the course. E-mails can be sent in either French or English at your convenience.</p> <p>Be mindful when sending e-mails. If it is your first contact with the instructor or TA, don't forget to present yourself. Try to clearly and concisely express your problem. Detail the steps you have already taken and what you have tried.</p> <p>Note</p> <p>For filtering reasons and easier e-mail management from the teaching staff, please make sure that all e-mail subjects you send are in the form: <code>[M1CHPS-TOP] &lt;your subject&gt;</code>.</p> <p>Any e-mail written in an informal manner or without proper syntax will be dismissed.</p> <p>Additionally, the HPCS Master has a Discord server on which students can reach the TA (handle: <code>@gabrl</code>), either by Private Message or through the <code>#techniques-optimisation-paralleles</code> channel.</p> <p> Invite link</p>"},{"location":"#material","title":"Material","text":""},{"location":"#lectures","title":"Lectures","text":"<p>Lecture slides are available here.</p>"},{"location":"#labs","title":"Labs","text":"<p>Lab material for this course is available on the following GitHub repository: TOP-25</p>"},{"location":"#resources","title":"Resources","text":"<p>Books:</p> <ul> <li>Comming soon...</li> </ul> <p>Standards, specifications &amp; documentation:</p> <ul> <li>MPI Standard</li> <li>OpenMP Specification</li> <li>C++ Reference</li> <li>Kokkos documentation</li> </ul> <p>Papers:</p> <ul> <li>Comming soon...</li> </ul> <p>Blogs/Articles:</p> <ul> <li>Daniel Lemire</li> <li>A (Draft) Taxonomy of SIMD Usage</li> <li>Comming soon...</li> </ul> <p>Useful tools:</p> <ul> <li>Compiler Explorer</li> <li>C++ Insights</li> <li>Spack PM</li> </ul>"},{"location":"lab1/","title":"Lab 1: Toolchains and HPC environment","text":"<p>A reminder on compilers, build systems, developement tools and profilers for HPC.</p> <p>The material for this lab is available on the following GitHub repo.</p>"},{"location":"lab1/#cmake-build-system","title":"CMake build system","text":"<ol> <li> <p>Write a minimal <code>CMakeLists.txt</code> file for the <code>vector</code> code.</p> </li> <li> <p>Improve your build system by splitting the <code>CMakeLists.txt</code> into multiple files in the directory hierarchy. Use CMake best practices.</p> </li> </ol> <p>Relevant resources for this exercise:</p> <ul> <li>Modern CMake guide</li> <li>CMake's <code>target_sources</code> command</li> <li>Examplar CMake</li> </ul>"},{"location":"lab1/#compiler-toolchains-and-flags","title":"Compiler toolchains and flags","text":"<ol> <li> <p>Using the GCC compiler, build <code>vector</code> into an executable without any optimization flags. What do you notice on the display of the values of vector <code>v3</code>?</p> </li> <li> <p>Add the compilation flags <code>-Wall</code>, <code>-Wextra</code> and <code>-Werror</code>. Fix all the warnings emitted by the compiler and recompile the program.</p> </li> <li> <p>Run the corrected program and record the time displayed.</p> </li> <li> <p>Compile different versions of the code using flags <code>-O1</code>, <code>-O2</code>, <code>-O3</code> and <code>-Ofast</code>. Run them each and record their execution time. What do you see? Compare the generated assembly.</p> </li> <li> <p>Repeat with a different compiler (e.g. LLVM Clang, Intel ICX, etc.). What differences do you notice?</p> </li> </ol>"},{"location":"lab1/#compilation-passes","title":"Compilation passes","text":"<ol> <li> <p>Compare the flags included with <code>-O2</code> on GCC 11.x and on GCC 12.x. Which flags differ?</p> </li> <li> <p>Compare the flags included with <code>-Ofast</code> on GCC and on LLVM Clang. Which flags differ?</p> </li> <li> <p>Study the program in the <code>saxpy</code> directory, then compile it with the <code>-fdump-tree-all</code> flag. Run the <code>ls</code> command. What do you see?</p> </li> <li> <p>What does it correspond to? How many do you see?</p> </li> <li> <p>Delete the files that have appeared, and recompile with the <code>-O1</code> flag. What do you observe? Are any files missing from the previous compilation?</p> </li> </ol>"},{"location":"lab1/#code-sanitizers","title":"Code sanitizers","text":"<p>Valgrind is an open-source programming tool for debugging, profiling and identifying memory leaks. (see Wikipedia page)</p> <p>AddressSanitizer (or <code>ASan</code>) is an open-source programming tool that detects memory corruption bugs such as buffer overflows or accesses to a dangling pointer (use-after-free). AddressSanitizer is based on compiler instrumentation and directly mapped shadow memory. AddressSanitizer is currently implemented in Clang (starting from version 3.1), GCC (starting from version 4.8), Xcode (starting from version 7.x) and MSVC (widely available starting from version 16.9). (see Wikipedia page)</p> <ol> <li> <p>Use Valgrind on the <code>saxpy</code> code. What do you see?</p> </li> <li> <p>Use ASan on the <code>saxpy</code> code. What do you observe? Fix the problem(s).</p> </li> <li> <p>What are <code>dhat</code>, <code>memcheck</code>, <code>cachegrind</code>, <code>callgrind</code>, <code>massif</code>?</p> </li> <li> <p>What are <code>kcachegrind</code>, <code>massif-visualizer</code>?</p> </li> </ol>"},{"location":"lab1/#performance-profiling-with-linux-perf-and-hotspot","title":"Performance profiling with Linux <code>perf</code> and <code>hotspot</code>","text":"<p>Gprof is a GNU Binary Utilities program for code profiling. When compiling and linking source code with GCC, simply add the <code>-pg</code> option, the program generates a <code>gmon.out</code> file containing the profiling information. You can then use Gprof to read this file, specifying the options.</p> <p>Perf (sometimes called <code>perf_events</code> or <code>perf-tools</code>, originally Performance Counters for Linux, PCL) is a performance analysis tool for Linux, available in the Linux kernel since version 2.6.31 in 2009. The userspace controlling utility, named <code>perf</code>, is accessed from the command line and provides a number of subcommands; it is capable of statistical profiling of the entire system (both kernel and userspace code).</p> <p>Hotspot is a GUI for the Linux Perf profiler that replaces the <code>perf report</code> command. (see GitHub page)</p> <p>The <code>mol-dyn</code> code supplied is a model of a molecular dynamics simulation in a gas (interaction between gas molecules). To compile, there are three preset sizes to choose from:</p> Preset CMake command Number of particles mini <code>-DNPART=MINI</code> 1372 medium <code>-DNPART=MEDIUM</code> 4000 maxi <code>-DNPART=MAXI</code> 13500 <ol> <li> <p>Use Gprof on the <code>mol-dyn</code> code. What is the most computationally-intensive function?</p> </li> <li> <p>Use Perf or Hotspot on the code. What is the most computationally-intensive function?</p> </li> <li> <p>What is a hotspot in the context of performance profiling?</p> </li> </ol>"},{"location":"lab1/#debuggers","title":"Debuggers","text":"<p>Study the program in <code>bugs</code>, then compile it with the <code>-g3</code> flag. Run the program using the GNU Debugger (GDB): Bash<pre><code>gdb &lt;BIN_NAME&gt;\n</code></pre></p> <p>GDB features interactive help. Start by browsing the help menu, typing <code>help</code> to get a list of commands. Then, <code>help</code> followed by a command to obtain information about it.</p> <ol> <li> <p>To locate the source of the first bug, type <code>run</code> (or <code>r</code>) under GDB. Quit (<code>quit</code>), correct the error and recompile the program.</p> </li> <li> <p>Proceed in the same way to correct the next bug. Use the <code>backtrace</code> (or <code>bt</code>) command to display the call stack and obtain more information on the source of the error.</p> </li> <li> <p>Identify the next error after recompiling the program. What is the problem? Can it be corrected?</p> </li> <li> <p>We're now going to solve the last bug with other basic GDB functions. Start the debugger without using the <code>run</code> command for the moment. Set a breakpoint on the <code>launch_fibonacci()</code> function using <code>breakpoint launch_fibonacci</code> (or <code>b launch_fibonacci</code>). Now try to print the value of <code>fibo_values-&gt;max</code>. What do you see? Use <code>up</code> to position yourself before the function call, then <code>list</code> to display the lines of code around the breakpoint. Now enter the command <code>print fibo_values</code>. Correct the problem and recompile the program.</p> </li> </ol> <p>The sequence is incorrect. We should obtain the following numbers: $$ F_0 = 0, F_1 = 1, F_2 = 1, F_3 = 2, F_4 = 5, F_6 = 8, \\ldots $$ To pinpoint the source of the error, we will display the values in the sequence step by step, monitoring changes to the <code>fibo_values-&gt;result</code> variable.</p>"},{"location":"lab1/#parallel-debugging","title":"Parallel debugging","text":"<p>There are debuggers specifically designed for the needs of parallel programs. Examples include TotalView and Linaro DDT. Although these programs are very powerful, they are not free nor open-source. Nevertheless, it is possible to use GDB to debug small-scale parallel programs. For multi-threaded programs, simply use the <code>info threads</code> command in GDB. We can choose to view a particular thread with the command <code>thread &lt;thread_number&gt;</code>.</p> <p>For MPI programs, we can use the following trick: Bash<pre><code>mpirun -np &lt;NPROC&gt; &lt;TERM_EMULATOR&gt; -e gdb -args &lt;MY_BIN&gt; [ARGS...]\n</code></pre> This will open one terminal per MPI process.</p> <ol> <li> <p>Start from the original <code>mol-dyn</code> code and parallelize the program's most expensive function using OpenMP or Pthread.</p> </li> <li> <p>If necessary, use the above methods to debug your parallel code on <code>mol-dyn</code>.</p> </li> </ol>"},{"location":"lab1/#hpc-package-managers","title":"HPC package managers","text":"<ul> <li>Spack PM</li> <li>GNU GUIX</li> </ul>"},{"location":"lab2/","title":"Lab 2: Methodology for measuring and visualizing performance metrics","text":"<p>An overview of performance metrics used in HPC, and how to correctly visualize them.</p> <p>Excellent resources from Torsten Hoefler (Gordon Bell Prize winner) on Scientific Benchmarking of Parallel Computing Systems:</p> <ul> <li>Paper</li> <li>Slides</li> <li>Recorded talk</li> </ul>"},{"location":"lab2/#scalability","title":"Scalability","text":"<p>In the most general sense, scalability (or scaling) is defined as \"the ability to handle more work as the amount of computing resources grows\". For software, scalability is sometimes referred to as parallelization efficiency \u2014 the ratio between the actual speedup and the ideal speedup obtained when using a certain number of processors. The speedup in parallel computing can be straightforwardly defined as:</p> \\[ Speedup = \\frac{t_1}{t_N} \\] <p>Where \\(t_1\\) is the computational time for running the software with one core (i.e. sequentially), and \\(t_N\\) is the computational time running the same software with N cores or processors. Ideally, we want software to have a linear speedup that is equal to the number of processors (i.e. \\(Speedup = N\\)). Unfortunately, this is a very challenging goal for real world applications to attain (see Amdhal's Law).</p> <p>Scalability testing measures the ability of an application to perform well or better with varying problem sizes and numbers of processors. It does not test the applications general funcionality or correctness.</p>"},{"location":"lab2/#strong-scaling","title":"Strong scaling","text":"<p>In case of strong scaling, the number of processors is increased while the problem size remains constant. This results in a reduced workload per processor as the amount of accessible parallelism increases.</p> <p>Strong scaling is mostly used for long-running CPU-bound applications to find a setup which results in a reasonable runtime with moderate resource costs. The individual workload must be kept high enough to keep all processors fully occupied. The speedup achieved by increasing the number of processes usually decreases more or less continuously.</p> <p>Strong scaling speedup can be calculated as:</p> \\[ Speedup = \\frac{t_1}{t_N} \\]"},{"location":"lab2/#weak-scaling","title":"Weak scaling","text":"<p>In case of weak scaling, both the number of processors and the problem size are increased. This results in a constant workload per processor.</p> <p>Weak scaling is mostly used for large memory-bound applications where the required memory cannot be satisfied by a single node. They usually scale well to higher core counts as memory access strategies often focus on the nearest neighboring nodes while ignoring those further away and therefore scale well themselves. The upscaling is usually restricted only by the available resources or the maximum problem size.</p> <p>Weak scaling efficiency can be calculated as:</p> \\[ Efficiency = \\frac{t_1}{t_N} \\]"},{"location":"lab2/#metrics","title":"Metrics","text":"<p>Some metrics used in HPC:</p> <ul> <li>Time</li> <li>Wall-clock time</li> <li>Clock cycles</li> <li>Computer processing efficiency</li> <li>IPC, latency, throughput</li> <li>IPS, FLOPS</li> <li>Arithmetic intensity</li> <li>Bandwidth</li> </ul> <p>Do not forget about measurement accuracy! Benchmarks you conduct to measure specific metrics should be run multiple times and the samples results should be analyzed:</p> <ul> <li>Minimum and maximum</li> <li>Mean/average</li> <li>Median</li> <li>Deviation/error</li> </ul>"},{"location":"lab2/#performance-data-visualization","title":"Performance data visualization","text":"<p>Some (non-exhaustive) advice to make good plots:</p> <ul> <li> <p>Always start your y-axis at zero.</p> <p>\"if zero is not the start, the truth falls apart\"</p> </li> <li> <p>Report standard deviation/error: this makes it clear if your measurements are stable or not.</p> </li> <li>Avoid log/log axes: they make the plot harder to read and obscure small values (only exception is strong scaling speedup).</li> <li>Know when to use log scale (particularly on the y-axis): if your data spans several orders of magnitude, it is easier to read and avoids compressing small data points at the bottom of the graph.</li> <li>Choose the most relevant unit for your axes.</li> <li>Whenever possible, avoid expressing labels in powers of 2 as they make it harder to grasp the actual value.</li> <li>Use consistent ranges of values on your axes, particularly when grouping subplots together.</li> <li>Vary your line- and marker-style to visually differentiate data (makes your plot color-blind- and black&amp;white- friendly too).</li> <li>When plotting strong/weak scaling performance, do not forget about the \"ideal\" line.</li> </ul>"},{"location":"lab2/#what-not-to-do","title":"What not to do","text":"<p>Examples of bad plots:</p> Example #1 - Weak Scaling Efficiency Example #2 - Parallel Normalized Runtime Example #3 - Reduction Normalized Runtime Example #4 - Execution Runtime Example #5 - Problem &amp; Strong Scaling Example #6 - Queue Pair Throughput Example #7 - MPI Call Intensity Example #8 - Strong Scaling of mini-app Example #9 - Strong Scaling of dot prod Example #10 - Baseline vs. Final Runtime"},{"location":"lab2/#stream-benchmarks","title":"STREAM benchmarks","text":"<p>Given the code provided in the lab 2 repository, you are to measure the performance of a very basic OpenMP implementation of the STREAM bandwidth benchmarks.</p>"},{"location":"lab2/#performance-metrics","title":"Performance metrics","text":"<ol> <li>Modify the <code>src/bin/main.cpp</code> file in order to actual measure something (e.g. execution time) and extract some    performance data about them. You can use standard C or C++ clocks, CPU clock cycles, or any other    unit of time that you think makes sense. You can also swap out the minimal provided code and use a dedicated    benchmarking library instead, e.g. Google Benchmarks,    nanobench, or Catch2.</li> <li>Derive some meaningful metrics (e.g., memory bandwith) from your raw measurements.</li> <li>Plot the obtained data (e.g., using a Python script).</li> </ol>"},{"location":"lab2/#scalability_1","title":"Scalability","text":"<ol> <li>Measure the strong scaling speedup and weak scaling efficiency of the STREAM benchmarks.    Write a simple script to do it.</li> <li>Plot the obtained data.</li> </ol>"},{"location":"lab3/","title":"Lab 3: Memory and data structure layout","text":"<p>Software engineering techniques to improve spatial and temporal data locality in the pursuit of performance.</p> <p>The code for the exercices is available at github.com/dssgabriel/TOP-25/lab3.</p>"},{"location":"lab3/#data-structure-packing","title":"Data structure packing","text":"<p>The goal of this exercice is to improve the layout of the data structures used in the <code>mesh</code> code. We start by packing to reduce its memory footprint.</p> <ol> <li> <p>Given the alignment rules imposed on structures/classes by the ISO C and C++ standards, determine how many bytes are used by the <code>Cell</code> structure.</p> </li> <li> <p>Rearrange the layout of the <code>Cell</code> structure and delete any unnecessary fields to minimize its memory footprint. Compute how much memory is saved using the optimized layout.</p> </li> <li> <p>Using Perf, check if there is a change in the number of cache misses between the two structure implementations.</p> </li> <li> <p>Measure the execution time difference between both implementations of the <code>Cell</code> structure.</p> </li> </ol> <p>C++ Insights can also show the padding inserted by the compiler in your data structures by enabling the <code>Show padding information</code> (see an example here).</p>"},{"location":"lab3/#array-of-structures-aos-vs-structure-of-arrays-soa","title":"Array of Structures (AoS) vs. Structure of Arrays (SoA)","text":"<p>Next, we try to improve memory access patterns of the <code>mesh</code> code by switching AoS to SoA.</p> <ol> <li> <p>Use your prefered tool to determine if the innermost loop in the <code>Mesh::compute_velocity()</code> function is correctly vectorized. You can analyze the assembly directely using a disassembler (e.g. <code>objdump</code>), or use tools such as Perf or MAQAO (CQA module).</p> </li> <li> <p>Measure the strong scaling of the current code. Write a small script (can be in bash, Python, R, etc.) to plot the results.</p> </li> <li> <p>Update the <code>mesh</code> code to use SoA instead of AoS.</p> </li> <li> <p>Use the same tools as in question 1 to assert that the change has helped the compiler vectorize the code.</p> </li> <li> <p>Measure the strong scaling of the new code.</p> </li> </ol> <p>The rest of this lab is your first lab assignment. Your goal is to improve the performance of a naive matrix product by tuning array layout and memory access patterns.</p> <p>For this exercise, we will rely on Kokkos and its Views, lightweight multidimensional array containers for C++. We will also leverage Kokkos' parallel constructs to express portable parallelism.</p> <p>If you have a GPU (NVIDIA w/ CUDA, AMD w/ HIP, or Intel Arc w/ SYCL), you can also change the Memory/Execution spaces of the code to trivially make it run on your accelerator (see how here).</p>"},{"location":"lab3/#array-layouts","title":"Array layouts","text":"<ol> <li> <p>Measure the performance of the naive matrix product and plot the results. You can simply measure its runtime, or derived metrics such as computational performance (in FLOP/s).  Conduct a strong scaling study and plot the results too.</p> </li> <li> <p>Try changing the layout of the matrices (e.g. <code>Kokkos::LayoutRight</code> to <code>Kokkos::LayoutLeft</code>). Do you see any performance improvement? Which combination gives the best results? Explain why.</p> </li> <li> <p>Repeat the steps of question 1.</p> </li> </ol>"},{"location":"lab3/#cache-blocking","title":"Cache blocking","text":"<ol> <li> <p>Profile the <code>matrix-product</code> code using Perf. Specifically, look at cache misses in the L1 and L3 (LLC) caches.</p> </li> <li> <p>Implement a cache-blocked version of the matrix multiplication to improve the spatial and temporal data locality of the loop.</p> </li> <li> <p>Assert that your implementation computes the correct result. Update the <code>CMakeLists.txt</code> file accordingly.</p> </li> <li> <p>Tune the block dimensions to maximize the performance of your cache-blocked version. Use Perf to assert the improvements in spatial and temporal locality.</p> </li> <li> <p>Make a final performance and strong scaling study of the code and plot the results. Conclude.</p> </li> </ol>"},{"location":"lab3/#deliverables","title":"Deliverables","text":"<p>You shall write a well-documented report explaining in detail your optimizations on storage layouts and cache blocking of the matrix product.  You are expected to present the profiling steps you have taken, the modification you have done to the code (and the reasons why), as well as your methodology for asserting the improvements. Your report must present plots that show the performance improvements for each version of the code, as well as the associated scaling studies (strong or weak, or both). Please include, in the report, a link to the GitHub repository with your code modifications.  It is strongly advised to write your report in LaTeX or Typst.</p> <p>The deadline for submitting the report is set to the 2025-04-23 at 23:59 CEST (Central European Summer Time).  Send it as a PDF file by email at gabriel.dossantos@cea.fr.</p> <p>Warning</p> <p>Reports submitted after the deadline or in the wrong file format will be dismissed automatically, and your grade will be 0.</p>"},{"location":"lab4/","title":"Lab 4: Shared-memory parallelism","text":"<p>An overview of common problems and optimization techniques for shared-memory parallelism.</p>"},{"location":"lab4/#false-sharing","title":"False sharing","text":"<p>In this exercise, you'll investigate the performance impact of false sharing in a multi-threaded C++ code snippet and implement solutions to mitigate this problem.</p>"},{"location":"lab4/#analyzing-the-problem","title":"Analyzing the problem","text":"<ol> <li> <p>Explain what is false sharing and why it might be occuring in this code.</p> </li> <li> <p>On a typical x86-64 architecture, what is the typical cache line size? How does this relate to the <code>counters</code> array layout?</p> </li> <li> <p>Compile and run the provided code with optimization flags.</p> </li> <li> <p>Try running with a different numbers of threads. How does performance scale? Does it scale linearly with the number of threads as you can expect?</p> </li> <li> <p>Use a profiling tool to measure cache-related metrics. What patterns do you observe?</p> </li> </ol>"},{"location":"lab4/#fixing-the-issue","title":"Fixing the issue","text":"<ol> <li> <p>Implement at least two different solutions to eliminate false sharing.</p> </li> <li> <p>For each solution, measure and report the performance improvements.</p> </li> <li> <p>Look into <code>std::hardware_destructive_interference_size</code> as a third solution. Measure and report its gains.</p> </li> </ol>"},{"location":"lab4/#parallel-scan","title":"Parallel scan","text":"<p>In this exercise, you'll explore different approaches to implementing parallel prefix sums (scans) and analyze their performance and scalability characteristics.</p> <p>While reduction operations combine all elements of a sequence into a single value, scan operations compute all the partial reductions. For example, given an array <code>[1, 2, 3, 4, 5]</code>, an inclusive scan produces <code>[1, 3, 6, 10, 15]</code> where each element is the sum of all previous elements including itself. An exclusive scan produces <code>[0, 1, 3, 6, 10]</code> where each element is the sum of all previous elements excluding itself.</p> <p>Scans are fundamental parallel primitives with applications in numerous algorithms including sorting, lexical analysis, and string comparison.</p> <p>Consider the sample code given in <code>integer-scan</code>.</p>"},{"location":"lab4/#sequential-implementation","title":"Sequential implementation","text":"<ol> <li> <p>Implement the inclusive scan operation sequentially and measure its performance in cycles/element using the <code>getticks()</code> function provided in the <code>\"cycle.h\"</code> header.</p> </li> <li> <p>Unlike the reduction operation, what is the theoretical computational complexity of a scan? How does this affect parallel implementation strategies?</p> </li> <li> <p>Compile and run your sequential implementation with both -O0 and -O3 optimization flags. Compare the performance results.</p> </li> </ol>"},{"location":"lab4/#parallelization","title":"Parallelization","text":"<ol> <li> <p>Parallelize the inclusive scan using OpenMP with just <code>#pragma omp parallel for</code>. Verify your results against the sequential version.</p> </li> <li> <p>Why does this approach fail to produce correct results? Explain the data dependency pattern in scan operations that makes them more challenging to parallelize than reductions.</p> </li> </ol>"},{"location":"lab4/#work-efficient-approach","title":"Work-efficient approach","text":"<ol> <li> <p>Implement the work-efficient parallel scan algorithm, which consists of an up-sweep (reduction) phase and a down-sweep phase.</p> </li> <li> <p>Use OpenMP tasks or sections to parallelize each phase. Note that synchronization between phases is critical.</p> </li> <li> <p>How does the performance compare to the sequential version? What is the theoretical speedup limit for this algorithm?</p> </li> </ol>"},{"location":"lab4/#going-deeper","title":"Going deeper...","text":"<ol> <li> <p>In the naive parallelization attempt, one could try to fix the data race using atomic operations. Update your implementation to use <code>#pragma omp atomic</code>. Is this approach viable for scan operations? Compare its performance with the other implementations. Why are atomic operations generally less effective for scan operations than for reductions?</p> </li> <li> <p>Implement a hybrid approach that combines the work-efficient and blocked methods for larger arrays. What are the trade-offs in choosing block sizes? How does this affect performance across different architectures?</p> </li> <li> <p>Implement a blocked parallel scan where:</p> </li> <li>The array is divided into blocks;</li> <li>Each block computes its local scan in parallel;</li> <li>The last elements of each block are scanned;</li> <li> <p>The results are propagated back to update each block.</p> </li> <li> <p>What are the limitations of your implementations if you plan to deploy them on many-core architectures like GPUs? How would you need to adapt your algorithms for GPU implementation using frameworks like CUDA or OpenCL? Propose (but do not implement) a GPU-friendly parallel scan algorithm that addresses these limitations.</p> </li> </ol>"},{"location":"lab5/","title":"Lab 5: MPI optimizations","text":""},{"location":"lab5/#generalities","title":"Generalities","text":"<ol> <li> <p>Give the definition of network latency.</p> </li> <li> <p>Give the definition of network bandwidth.</p> </li> <li> <p>Give their respective units.</p> </li> <li> <p>Give the name and version of the MPI library you are using. Give the command used to retrieve this information.</p> </li> <li> <p>To make sure your installation works, you can use the hostname command, which returns the name of the current    process\u2019s host:    Bash<pre><code>mpirun -np 2 hostname\n</code></pre>    What happens when all processes are executed on the same host?</p> </li> </ol>"},{"location":"lab5/#ping-pong","title":"Ping pong","text":""},{"location":"lab5/#set-up","title":"Set up","text":"<p>In this exercise, you will do performance measurements using the ping-pong benchmark to familiarize with MPI\u2019s characteristics communication times. Start by writing a ping-pong between two MPI processes. Rank 0 will send a message, and rank 1 will receive it. Write your code so that you can set the message size as a command-line argument.</p> <ol> <li> <p>The code hereafter is a simple ping from rank 0 to rank 1 (no answer). We have voluntarily added a <code>sleep(2)</code> in    the rank 1\u2032s code.    C<pre><code>if (rank == 0) {\n  t0 = MPI_Wtime();\n  MPI_Send(buffer, size, MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n  t1 = MPI_Wtime();\n  printf(\"%lu\\t%g\\n\", size, t1 - t0);\n} else if (rank == 1) {\n  sleep(2);\n  MPI_Recv(buffer, size, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n}\n</code></pre>    Using OpenMPI 2.0.1, we measured the execution time as a function of message size. We obtain 53.1654 ms for a size    of 4,000 Bytes, and 2.0006 s for a size of 4,096 Bytes.    How do you explain this difference? What did we actually measure?</p> </li> <li> <p>Find the message size for which this gap appears in your MPI implementation. This size might be different than the    one we observed using OpenMPI 2.0.1 (4,096 B). Explain the reason why.</p> </li> <li> <p>We decide to remove the call to sleep() . Does the measure make sense now? Why?</p> </li> <li> <p>Propose an alternative to measure the actual sending time of a message (i.e. the time it took to actually receive it    on the target rank). Write the corresponding program.     It is strongly advised that you read the MPI documentation of the different modes of communications: Standard,     Buffered, Ready, Synchronous, etc.</p> </li> <li> <p>Write an actual MPI ping-pong (rank 1 answers back to rank 0). Explain why the measured times corresponds to a    message exchange.</p> </li> </ol>"},{"location":"lab5/#first-measures","title":"First measures","text":"<ol> <li> <p>Do multiple measures with multiples of 32 Bytes for the message sizes. What do you see?</p> </li> <li> <p>We propose adding a barrier before the message exchange phase. Explain what is the interest of this barrier with    regards to the accuracy of the measurements.</p> </li> <li> <p>Rerun the measures of question 11. Are the changes from the previous question su\ufb03cient? Why?</p> </li> </ol>"},{"location":"lab5/#repetitions","title":"Repetitions","text":"<p>When taking a measurement, it is important to remember that our environment does not allow us to reproduce the exact conditions between each run. Moreover, time measurement itself is fraught with uncertainty. To solve this problem, we prefer to repeat measurements and calculate an average (mean, think about which one is the best) and a median. Alternatively, we can repeat runs until, e.g., the 95% confidence interval is within 5% of our reported means.</p> <ol> <li>Update the program to add repetitions. You should now print the average execution time.</li> </ol>"},{"location":"lab5/#impact-of-message-size","title":"Impact of message size","text":"Fig. 1 - Communication time as a function of message size <p>Fig. 1 shows the evolution of ping-pong time according to message size between intra-node (local) and inter-node (Ethernet and Infiniband) exchanges. Scales are logarithmic.</p> <ol> <li> <p>Explain why there is such a big gap between local and In\ufb01niband for small sizes.    Explain this gap shrinks progressively once the message size increases.</p> </li> <li> <p>From the previous results, should we send distinct sets of data separately? Or should we try to group them together    Is it true for all sizes?</p> </li> </ol>"},{"location":"lab5/#collectives-and-algorithms-in-openmpi","title":"Collectives and algorithms in OpenMPI","text":"<p>Using a recent version of OpenMPI (4.0.5+), run the command <code>ompi_info -all</code>. This gives you multiple informations about your installation. We will use to know which algorithms are available in the <code>coll tuned</code> module of OpenMPI, where blocking collectives are implemented.</p> <ol> <li> <p>Find what are the usable algorithms for the <code>MPI_Bcast</code> routine. Compare their performance depending on the number of    MPI processes and buffer size.</p> </li> <li> <p>Find what are the usable algorithms for the <code>MPI_Gather</code> routine. Compare their performance depending on the number    of MPI processes and buffer size.</p> </li> <li> <p>Find what are the usable algorithms for the <code>MPI_Reduce</code> routine. Compare their performance depending on the number    of MPI processes and buffer size.</p> </li> <li> <p>Find what are the usable algorithms for the <code>MPI_Alltoall</code> routine. Compare their performance depending on the    number of MPI processes and buffer size.</p> </li> <li> <p>For each collective, why do we need multiple algorithms?</p> </li> </ol>"},{"location":"lab5/#experimental-evaluation-of-scalability","title":"Experimental evaluation of scalability","text":"<p>Let\u2019s start with a very simple benchmark. Our MPI program will initially make no communication. Have node 0 measure the program\u2019s execution time. It will perform a simple sum of two vectors, in the following form: $$ X_i^{t+1} = X_i^t + c $$ with \\(X\\) a vector of size \\(N\\) and \\(c\\) a real constant.</p> <p>Clearly, such an equation can be distributed over several MPI processes without any communication, so we will simply implement the sum method and execute it in a loop to obtain a suitable execution time for our measurement.</p> <ol> <li>Implement this benchmark like so:</li> <li>Initialize the MPI context</li> <li>Pass the vector size and the number of repetitions as parameters of the program</li> <li>Allocate and initialize memory for two vectors. Each node shall compute the sum on a vector of size \\(\\frac{N}{nb_{tasks}\\)</li> <li> <p>Make sure to measure the execution time as seen previously in this lab</p> </li> <li> <p>What does strong scalability represent?</p> </li> <li> <p>What does weak scalability represent?</p> </li> <li> <p>To introduce communications in our program, we want to consider a case inspired by finite elements method (FEM).    Our equation is now:    $$    X_i^{t+1} = \\frac{X_{i-1}^\ud835\udc61 + 2 X_i^t + X_{i+1}^t}{4}    $$    Here, MPI slicing requires the addition of communications for border elements. Modify the application in this way    and evaluate scalability.</p> </li> <li> <p>Introduce a global synchronization in the repetitions loop and re-evalute the application\u2019s scalability. What do you    see? What remarks can you make about communications and the use of barriers in MPI applications? What should you do    to avoid this problem?</p> </li> </ol>"},{"location":"repository_and_tools/","title":"Repository and tools","text":"<p>The lab material for this course is available on the following GitHub repository.</p> <p>Most of the software used throught in this course can be easily installed via your Linux distribution package manager (e.g. <code>apt</code> on Ubuntu/Debian, <code>dnf</code> on RedHat-like, <code>pacman</code>/<code>paru</code> on Arch-like, etc.), or using the Spack Package Manager.</p> <p>To load Spack-installed software, use the <code>spack load &lt;PACKAGE&gt;</code> command.</p>"},{"location":"repository_and_tools/#required-software","title":"Required software","text":""},{"location":"repository_and_tools/#cmake","title":"CMake","text":"<p>Most of this course material relies on CMake for building the code provided in each exercise.</p> <p>You can get CMake from here. The prefered version for this course is 3.31, the minimum required version is 3.25.</p> <ol> <li> <p>Download the binary distribution corresponding to your OS. Prefer the tarball (<code>.tar.gz</code>) instead of the shell script.</p> </li> <li> <p>Unpack the tarball where you have downloaded it: Bash<pre><code>tar xzf cmake-3.31.6-linux-x86_64.tar.gz\n</code></pre></p> </li> <li> <p>Make the CMake executable available in your <code>PATH</code>:</p> BashZSHFish Bash<pre><code>printf 'export PATH=\"%s/cmake-3.31.6-linux-x86_64/bin:$PATH\"\\n' $(pwd) &gt;&gt; $HOME/.bashrc\nsource $HOME/.bashrc\n</code></pre> Bash<pre><code>printf 'export PATH=\"%s/cmake-3.31.6-linux-x86_64/bin:$PATH\"\\n' $(pwd) &gt;&gt; $HOME/.zshrc\nsource $HOME/.zshrc\n</code></pre> Fish<pre><code>printf 'set --export --prepend PATH %s/cmake-3.31.6-linux-x86_64/bin\\n' $(pwd) &gt;&gt; $XDG_CONFIG_HOME/fish/config.fish\nsource $XDG_CONFIG_HOME/fish/config.fish\n</code></pre> </li> </ol>"},{"location":"repository_and_tools/#compilers","title":"Compilers","text":"Compiler Version Spack command GNU Compiler Collection (gcc) 14.2 <code>spack install gcc@14.2.0</code> LLVM Clang (clang) 20.1.0 <code>spack install llvm@20.1.0</code> Intel oneAPI (icx) 2025.1 <code>spack install intel-oneapi-compilers@2025.0.4</code>"},{"location":"repository_and_tools/#mpi","title":"MPI","text":"Implementation Version Spack command Open MPI 5.0.6 <code>spack install openmpi@5.0.6</code> MPICH 4.2.3 <code>spack install mpich@4.2.3</code>"},{"location":"repository_and_tools/#formatting-and-style","title":"Formatting and style","text":"<p>We expect students to format their code according to a <code>clang-format</code> spec (one is provided at the root of the lab repo). Similarly, CMake files may be formatted using tools such as <code>gersemi</code>.</p> <p>To improve your C/C++ code, we suggest you to regularly run tools such as <code>clang-tidy</code>, <code>sonar-lint</code> or other code linters that help detect anti-patterns, weird or inefficient coding styles, and suggest ways of writing more idiomatic C/C++.</p> <p>You can also take a look at popular and battle-tested code style guidelines such as:</p> <ul> <li>C++ Core Guidelines</li> <li>Google C++ Style Guide</li> <li>Carbon C++ Style Guide</li> <li>MISRA C++</li> <li>ANSSI C Rules for C language software development</li> </ul>"}]}